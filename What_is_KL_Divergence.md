# What is KL divergence

**Translated from https://www.jianshu.com/p/43318a3dc715**

## What the hell is KL divergence

**Kullback-Leibler Divergence**, also called KL divergence is a method to quantifying the difference between two probability distributions P and Q, also known as relative entropy.


